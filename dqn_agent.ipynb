{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from simul_test import Sumobot\n",
    "\n",
    "import random, sys\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Adam\n",
    "import math\n",
    "env = Sumobot()\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "class DQN:\n",
    "\n",
    "    \"\"\" Implementation of deep q learning algorithm \"\"\"\n",
    "\n",
    "    def __init__(self, action_space, state_space): #making the agent\n",
    "\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "        self.epsilon = 1\n",
    "        self.gamma = .95\n",
    "        self.batch_size = 64\n",
    "        self.epsilon_min = .01\n",
    "        self.epsilon_decay = .997\n",
    "        self.learning_rate = 0.001\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.model = self.build_model()\n",
    "        self.episode_coords = [0,0] # will be a 2x2 matrix\n",
    "        self.angle = 0\n",
    "        self.arena_radius = 44\n",
    "        self.radius = -36\n",
    "        self.angle_delta = 30 \n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential() #groups a linear stack of layers into a model, provides training and inference features on this model\n",
    "        model.add(Dense(64, input_shape=(self.state_space,), activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(self.action_space, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        # once the random values chosen are starting to become larger\n",
    "        # than the epsilon, the algorithm will start choosing values from the \n",
    "        # matrix that it has learnt.\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_space) #choose a random move\n",
    "        act_values = self.model.predict(state) # Generates output predictions for the input samples.\n",
    "        return np.argmax(act_values[0]) \n",
    "\n",
    "    def replay(self):\n",
    "#         print(len(self.memory))\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states = np.array([i[0] for i in minibatch])\n",
    "        actions = np.array([i[1] for i in minibatch])\n",
    "        rewards = np.array([i[2] for i in minibatch])\n",
    "        next_states = np.array([i[3] for i in minibatch])\n",
    "        dones = np.array([i[4] for i in minibatch])\n",
    "\n",
    "        states = np.squeeze(states)\n",
    "        next_states = np.squeeze(next_states)\n",
    "\n",
    "        targets = rewards + self.gamma*(np.amax(self.model.predict_on_batch(next_states), axis=1))*(1-dones)\n",
    "        targets_full = self.model.predict_on_batch(states)\n",
    "\n",
    "        ind = np.array([i for i in range(self.batch_size)])\n",
    "        targets_full[[ind], [actions]] = targets\n",
    "\n",
    "        self.model.fit(states, targets_full, epochs=1, verbose = 0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    \n",
    "\n",
    "def train_dqn(episode):\n",
    "\n",
    "    loss = []\n",
    "    #sumobot and enemy action and state space\n",
    "    action_space = 9\n",
    "    state_space = 4\n",
    "    max_steps = 750\n",
    "    #sumobot agent\n",
    "    agent = DQN(action_space, state_space)\n",
    "    for e in range(episode):\n",
    "        print(e)\n",
    "        for i in range(3):\n",
    "            enemy_angle = 0\n",
    "            dist = [17, 34, 51]\n",
    "            enemy_angle_delta = 45\n",
    "            ex = 0\n",
    "            ey = 0\n",
    "            while(agent.angle < 360):\n",
    "                x = math.sqrt((agent.radius**2)/(1 + (math.tan(math.radians(agent.angle)))**2))\n",
    "                y = x * math.tan(math.radians(agent.angle))\n",
    "                if 0 <= agent.angle <= 90:\n",
    "                    agent.episode_coords[0] = [-x,-y]\n",
    "                elif 90 < agent.angle <= 180:\n",
    "                    agent.episode_coords[0] = [x,y]\n",
    "                elif 180 < agent.angle <= 270:\n",
    "                    agent.episode_coords[0] = [x,y]\n",
    "                elif 270 < agent.angle < 360:\n",
    "                    agent.episode_coords[0] = [-x,-y]\n",
    "                for r in dist:\n",
    "                    while(enemy_angle < 360):\n",
    "                        if enemy_angle <= 90 or enemy_angle > 270: \n",
    "                            ex = x - math.sqrt((r**2)/(1+(math.tan(math.radians(enemy_angle))**2)))\n",
    "                            ey = y - (ex - x) * math.tan(math.radians(enemy_angle)) \n",
    "                        else:\n",
    "                            ex = x + math.sqrt((r**2)/(1+(math.tan(math.radians(enemy_angle))**2)))\n",
    "                            ey = y + (ex - x) * math.tan(math.radians(enemy_angle))  \n",
    "                    \n",
    "                        if (math.sqrt(ex**2 + ey**2) < agent.arena_radius):\n",
    "                            if 0 <= enemy_angle <= 90:\n",
    "                                if enemy_angle == 90:\n",
    "                                    agent.episode_coords[1] = [ex,-ey-r]\n",
    "                                else:\n",
    "                                    agent.episode_coords[1] = [ex,-ey]\n",
    "                            elif 90 < enemy_angle <= 180:\n",
    "                                if enemy_angle == 180:\n",
    "                                    agent.episode_coords[1] = [ex-r,ey]\n",
    "                                agent.episode_coords[1] = [ex,ey]\n",
    "                            elif 180 < enemy_angle <= 270:\n",
    "                                if enemy_angle == 270:\n",
    "                                    if r == 17 :\n",
    "                                        agent.episode_coords[1] = [ex,ey+r]\n",
    "                                    else: \n",
    "                                        agent.episode_coords[1] = [ex,ey]\n",
    "                                else:\n",
    "                                    agent.episode_coords[1] = [ex,ey]\n",
    "                            elif 270 < enemy_angle <= 360:\n",
    "                                if enemy_angle == 315:\n",
    "                                    if r == 34:\n",
    "                                        agent.episode_coords[1] = [ex,-ey]\n",
    "                                    else:\n",
    "                                        agent.episode_coords[1] = [ex,-ey]\n",
    "                                else:\n",
    "                                    agent.episode_coords[1] = [ex,ey]\n",
    "                                    \n",
    "                                    \n",
    "                            state = env.reset(agent.episode_coords)\n",
    "                            state = np.reshape(state, (1, state_space))\n",
    "                            score = 0\n",
    "                            \n",
    "                            \n",
    "                            for i in range(max_steps):\n",
    "                                action = agent.act(state)\n",
    "                                reward, next_state, done = env.step(action)\n",
    "                                score += reward\n",
    "                                next_state = np.reshape(next_state, (1, state_space))\n",
    "                                agent.remember(state, action, reward, next_state, done)\n",
    "                                state = next_state\n",
    "                                agent.replay()\n",
    "                                if done:\n",
    "                    #                 print(\"episode: {}/{}, score: {}\".format(e, episode, score))\n",
    "#                                     agent.model.summary()\n",
    "                                    break\n",
    "#                             agent.model.summary()\n",
    "                            loss.append(score)\n",
    "                        else: \n",
    "                            pass\n",
    "                        enemy_angle += enemy_angle_delta\n",
    "                    enemy_angle -= 360     \n",
    "                agent.angle += agent.angle_delta\n",
    "            agent.angle -= 360 \n",
    "            agent.radius += 10 \n",
    "    #save both models\n",
    "    agent.model.save('modelV2.h5')\n",
    "    return loss\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "# So when the interpreter runs a module, the __name__ variable will be set as  __main__ if the module that is being run is the main program.\n",
    "# But if the code is importing the module from another module, then the __name__  variable will be set to that moduleâ€™s name.\n",
    "# https://www.freecodecamp.org/news/if-name-main-python-example/\n",
    "    ep = 2\n",
    "    loss = train_dqn(ep)\n",
    "#     print(loss)\n",
    "    plt.plot([i for i in range(ep)], loss)\n",
    "    plt.xlabel('episodes')\n",
    "    plt.ylabel('reward')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
